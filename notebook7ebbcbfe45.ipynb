{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11242560,"sourceType":"datasetVersion","datasetId":7024276}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Load the updated IPC dataset\ndf = pd.read_csv('/kaggle/input/dataset/Balanced_IPC_Sections_409_Cleaned.csv')\n\n# Check the first few rows to understand the structure\nprint(df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T16:18:19.970870Z","iopub.execute_input":"2025-04-01T16:18:19.971092Z","iopub.status.idle":"2025-04-01T16:18:21.651829Z","shell.execute_reply.started":"2025-04-01T16:18:19.971070Z","shell.execute_reply":"2025-04-01T16:18:21.650757Z"}},"outputs":[{"name":"stdout","text":"                                    Case_Description IPC_section\n0  Several individuals conspired to kidnap a busi...     IPC 120\n1  A group of officials conspired to manipulate t...     IPC 120\n2  Smugglers conspired to illegally transport end...     IPC 120\n3  Two employees of a company conspired to leak c...     IPC 120\n4  A terrorist group not only conspired but also ...     IPC 120\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T16:18:49.259048Z","iopub.execute_input":"2025-04-01T16:18:49.259340Z","iopub.status.idle":"2025-04-01T16:18:54.703520Z","shell.execute_reply.started":"2025-04-01T16:18:49.259319Z","shell.execute_reply":"2025-04-01T16:18:54.702450Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.17.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.29.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from transformers import DistilBertTokenizer\nfrom datasets import Dataset\n\n# Load the tokenizer\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n\n# Tokenize the text\ndef tokenize_function(examples):\n    return tokenizer(examples['Case_Description'], padding=True, truncation=True)\n\n# Convert the dataframe into a HuggingFace Dataset\ndataset = Dataset.from_pandas(df[['Case_Description', 'IPC_section']])\n\n# Apply tokenization\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n\n# Show tokenized data\nprint(tokenized_datasets)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T16:19:08.778586Z","iopub.execute_input":"2025-04-01T16:19:08.778882Z","iopub.status.idle":"2025-04-01T16:19:32.105157Z","shell.execute_reply.started":"2025-04-01T16:19:08.778856Z","shell.execute_reply":"2025-04-01T16:19:32.104073Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbff84ff9fb94a7f9e955483989f1ae7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0dd3624203c49ba98844fd889917c7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb780b515acb4105810053e39950fc4d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc72ce8340a94896a998d79df212eb20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/22495 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"044ab5391a2b4db9b3fc862381447384"}},"metadata":{}},{"name":"stdout","text":"Dataset({\n    features: ['Case_Description', 'IPC_section', 'input_ids', 'attention_mask'],\n    num_rows: 22495\n})\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from transformers import DistilBertForSequenceClassification\n\n# Define the model\nmodel = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(df['IPC_section'].unique()))\n\n# Check the model architecture\nprint(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T16:19:36.797729Z","iopub.execute_input":"2025-04-01T16:19:36.798059Z","iopub.status.idle":"2025-04-01T16:19:55.355261Z","shell.execute_reply.started":"2025-04-01T16:19:36.797996Z","shell.execute_reply":"2025-04-01T16:19:55.354470Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7c0d7c92b0d4fe79e569376584d88c3"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"DistilBertForSequenceClassification(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0-5): 6 x TransformerBlock(\n          (attention): DistilBertSdpaAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (classifier): Linear(in_features=768, out_features=409, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom transformers import Trainer, TrainingArguments, DistilBertForSequenceClassification, DistilBertTokenizer, DataCollatorWithPadding\nfrom datasets import Dataset\n\n# Disable WandB logging\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n# Load the dataset\ndf = pd.read_csv('/kaggle/input/dataset/Balanced_IPC_Sections_409_Cleaned.csv')\n\n# Tokenizer for DistilBERT\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n\n# Tokenize the dataset\ndef tokenize_function(examples):\n    return tokenizer(examples['Case_Description'], padding='max_length', truncation=True) # Pad to max_length\n\n# Convert to HuggingFace Dataset\ndataset = Dataset.from_pandas(df[['Case_Description', 'IPC_section']])\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n\n# Map IPC sections to numeric labels\ndf['label'] = df['IPC_section'].astype('category').cat.codes\n\n# Ensure that labels are present\ntokenized_datasets = tokenized_datasets.add_column('labels', df['label'].values)\n\n# Initialize the model\nmodel = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(df['IPC_section'].unique()))\n\n# Define training arguments with increased epochs and checkpoints\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    per_device_train_batch_size=8,\n    num_train_epochs=10,  # Increased epochs to 20\n    logging_dir='./logs',\n    logging_steps=200,\n    evaluation_strategy=\"epoch\",  # Logging after each epoch\n    save_strategy=\"steps\",  # Save model every specified steps\n    save_steps=500,  # Save model every 500 steps (you can adjust this as per your requirement)\n    save_total_limit=3,  # Limit the number of saved checkpoints to 3\n    report_to=\"none\",  # Disable WandB reporting\n)\n\n# Use DataCollatorWithPadding\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n# Initialize the trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets,\n    eval_dataset=tokenized_datasets,\n    data_collator=data_collator # Add data_collator\n)\n\n# Start training\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T16:51:14.983691Z","iopub.execute_input":"2025-04-01T16:51:14.984087Z","iopub.status.idle":"2025-04-01T19:17:38.777584Z","shell.execute_reply.started":"2025-04-01T16:51:14.984053Z","shell.execute_reply":"2025-04-01T19:17:38.776733Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/22495 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c5c36f871e743519310c1a3e457dd95"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='14060' max='14060' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [14060/14060 2:26:05, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.028900</td>\n      <td>1.593679</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.757500</td>\n      <td>0.504533</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.334500</td>\n      <td>0.196863</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.157100</td>\n      <td>0.063720</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.056000</td>\n      <td>0.019653</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.024700</td>\n      <td>0.008767</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.012400</td>\n      <td>0.008568</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.005600</td>\n      <td>0.005560</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.010500</td>\n      <td>0.005012</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.004400</td>\n      <td>0.005980</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=14060, training_loss=0.5667580621849592, metrics={'train_runtime': 8768.9148, 'train_samples_per_second': 25.653, 'train_steps_per_second': 1.603, 'total_flos': 3.00148271030784e+16, 'train_loss': 0.5667580621849592, 'epoch': 10.0})"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Evaluate the model\neval_results = trainer.evaluate()\nprint(eval_results)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T19:17:53.201527Z","iopub.execute_input":"2025-04-01T19:17:53.201837Z","iopub.status.idle":"2025-04-01T19:21:32.414872Z","shell.execute_reply.started":"2025-04-01T19:17:53.201814Z","shell.execute_reply":"2025-04-01T19:21:32.414069Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1406' max='1406' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1406/1406 03:39]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.005979881156235933, 'eval_runtime': 219.205, 'eval_samples_per_second': 102.621, 'eval_steps_per_second': 6.414, 'epoch': 10.0}\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T19:22:08.311820Z","iopub.execute_input":"2025-04-01T19:22:08.312163Z","iopub.status.idle":"2025-04-01T19:22:08.315776Z","shell.execute_reply.started":"2025-04-01T19:22:08.312135Z","shell.execute_reply":"2025-04-01T19:22:08.314861Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Example complaint text\ncomplaint_text = [\"The victim was assaulted by a known individual with intent to har.\"]\n\n# Tokenize the input text and move to the GPU\ninputs = tokenizer(complaint_text, return_tensors=\"pt\", truncation=True, padding=True).to(model.device) # Move inputs to the same device as the model\n\n# Make a prediction\noutputs = model(**inputs)\npredicted_ipc = torch.argmax(outputs.logits).item()\n\n# Map the numeric prediction back to the IPC section\npredicted_ipc_section = df['IPC_section'].astype('category').cat.categories[predicted_ipc]\nprint(f\"Predicted IPC Section: {predicted_ipc_section}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T19:29:08.052821Z","iopub.execute_input":"2025-04-01T19:29:08.053206Z","iopub.status.idle":"2025-04-01T19:29:08.070959Z","shell.execute_reply.started":"2025-04-01T19:29:08.053175Z","shell.execute_reply":"2025-04-01T19:29:08.070120Z"}},"outputs":[{"name":"stdout","text":"Predicted IPC Section: IPC 376\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}