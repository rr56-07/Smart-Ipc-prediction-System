{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11247774,"sourceType":"datasetVersion","datasetId":7028161},{"sourceId":321203,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":270816,"modelId":291803}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom collections import Counter\nfrom sklearn.preprocessing import LabelEncoder\nimport re\n\n# Load the dataset\nprint(\"Loading dataset...\")\ndf = pd.read_csv(\"/kaggle/input/ipc-section/Balanced_IPC_Sections_409_Cleaned.csv\")\nprint(f\"Dataset loaded with shape: {df.shape}\")\n\n# Check columns\nprint(\"\\nColumns in dataset:\")\nprint(df.columns.tolist())\n\n# Check data types\nprint(\"\\nData types:\")\nprint(df.dtypes)\n\n# Check for missing values\nprint(\"\\nMissing values per column:\")\nprint(df.isnull().sum())\n\n# Extract relevant columns\nprint(\"\\nExtracting 'Case_Description' and 'IPC_section' columns...\")\nif 'Case_Description' in df.columns and 'IPC_section' in df.columns:\n    analysis_df = df[['Case_Description', 'IPC_section']]\n    print(\"Successfully extracted relevant columns.\")\nelse:\n    print(\"Error: Required columns not found in dataset.\")\n    available_columns = df.columns.tolist()\n    print(f\"Available columns: {available_columns}\")\n    # Try to find columns with similar names\n    case_columns = [col for col in available_columns if 'case' in col.lower()]\n    ipc_columns = [col for col in available_columns if 'ipc' in col.lower() or 'section' in col.lower()]\n    if case_columns:\n        print(f\"Possible case description columns: {case_columns}\")\n    if ipc_columns:\n        print(f\"Possible IPC section columns: {ipc_columns}\")\n    # If no similar columns found, use the first two columns\n    if len(df.columns) >= 2:\n        analysis_df = df.iloc[:, :2]\n        print(f\"Using first two columns as fallback: {df.columns[0]} and {df.columns[1]}\")\n    else:\n        print(\"Dataset has fewer than 2 columns. Cannot proceed.\")\n        exit()\n\n# Rename columns if necessary\nif 'Case_Description' not in analysis_df.columns or 'IPC_section' not in analysis_df.columns:\n    print(\"Renaming columns for consistency...\")\n    column_names = analysis_df.columns.tolist()\n    analysis_df.columns = ['Case_Description', 'IPC_section']\n    print(f\"Renamed columns from {column_names} to ['Case_Description', 'IPC_section']\")\n\n# Check for empty strings or whitespace-only entries\nprint(\"\\nChecking for empty or whitespace-only entries...\")\nempty_case = analysis_df['Case_Description'].str.strip().eq('').sum()\nempty_ipc = analysis_df['IPC_section'].str.strip().eq('').sum()\nprint(f\"Empty Case_Description entries: {empty_case}\")\nprint(f\"Empty IPC_section entries: {empty_ipc}\")\n\n# Clean the data if needed\nif empty_case > 0 or empty_ipc > 0:\n    print(\"Cleaning empty entries...\")\n    analysis_df = analysis_df[analysis_df['Case_Description'].str.strip().ne('')]\n    analysis_df = analysis_df[analysis_df['IPC_section'].str.strip().ne('')]\n    print(f\"After cleaning, dataset shape: {analysis_df.shape}\")\n\n# Check IPC section distribution\nprint(\"\\nIPC Section Distribution:\")\nipc_counts = analysis_df['IPC_section'].value_counts()\nprint(f\"Number of unique IPC sections: {len(ipc_counts)}\")\nprint(f\"Top 10 most common IPC sections:\")\nprint(ipc_counts.head(10))\nprint(f\"Bottom 10 least common IPC sections:\")\nprint(ipc_counts.tail(10))\n\n# Label encoding simulation\nprint(\"\\nSimulating label encoding...\")\nlabel_encoder = LabelEncoder()\nlabels = label_encoder.fit_transform(analysis_df['IPC_section'])\nprint(f\"Number of unique encoded labels: {len(np.unique(labels))}\")\nprint(f\"Sample of encoded labels: {labels[:10]}\")\nprint(f\"Corresponding IPC sections: {analysis_df['IPC_section'].iloc[:10].tolist()}\")\n\n# Text data analysis\nprint(\"\\nCase Description Analysis:\")\ncase_lengths = analysis_df['Case_Description'].str.len()\nprint(f\"Average case description length: {case_lengths.mean():.2f} characters\")\nprint(f\"Minimum length: {case_lengths.min()} characters\")\nprint(f\"Maximum length: {case_lengths.max()} characters\")\n\n# Word count analysis\nword_counts = analysis_df['Case_Description'].apply(lambda x: len(str(x).split()))\nprint(f\"Average word count: {word_counts.mean():.2f} words\")\nprint(f\"Minimum word count: {word_counts.min()} words\")\nprint(f\"Maximum word count: {word_counts.max()} words\")\n\n# Check for outliers in case description length\nprint(\"\\nIdentifying outliers in case description length...\")\noutliers = analysis_df[word_counts > word_counts.quantile(0.99)]\nprint(f\"Number of cases with extremely long descriptions (>99th percentile): {len(outliers)}\")\nif len(outliers) > 0:\n    print(f\"Sample outlier (truncated): {outliers['Case_Description'].iloc[0][:200]}...\")\n\n# Check for special characters or non-ASCII characters\nprint(\"\\nChecking for special characters and non-ASCII characters...\")\nhas_special_chars = analysis_df['Case_Description'].apply(lambda x: bool(re.search(r'[^\\x00-\\x7F]', str(x))))\nspecial_char_count = has_special_chars.sum()\nprint(f\"Number of cases with non-ASCII characters: {special_char_count}\")\n\n# Print summary of findings\nprint(\"\\n====== SUMMARY OF FINDINGS ======\")\nprint(f\"Total number of samples: {len(analysis_df)}\")\nprint(f\"Number of unique IPC sections: {len(ipc_counts)}\")\nprint(f\"Average case description length: {case_lengths.mean():.2f} characters ({word_counts.mean():.2f} words)\")\nprint(f\"Most common IPC section: {ipc_counts.index[0]} with {ipc_counts.iloc[0]} occurrences\")\nprint(f\"Least common IPC section: {ipc_counts.index[-1]} with {ipc_counts.iloc[-1]} occurrences\")\nprint(f\"IPC section distribution balance: {'Balanced' if ipc_counts.max() / ipc_counts.min() < 5 else 'Imbalanced'}\")\nprint(\"===================================\")\n\n# Generate a plot of IPC section distribution (top 20)\nplt.figure(figsize=(15, 8))\ntop20_ipc = ipc_counts.head(20)\nsns.barplot(x=top20_ipc.index, y=top20_ipc.values)\nplt.title('Top 20 IPC Sections by Frequency')\nplt.xlabel('IPC Section')\nplt.ylabel('Count')\nplt.xticks(rotation=90)\nplt.tight_layout()\nplt.savefig('ipc_distribution_top20.png')\nprint(\"Plot of top 20 IPC sections saved as 'ipc_distribution_top20.png'\")\n\n# Generate a histogram of case description length\nplt.figure(figsize=(12, 6))\nplt.hist(word_counts, bins=50)\nplt.title('Distribution of Case Description Word Count')\nplt.xlabel('Word Count')\nplt.ylabel('Frequency')\nplt.axvline(word_counts.mean(), color='r', linestyle='dashed', linewidth=1)\nplt.text(word_counts.mean()*1.1, plt.ylim()[1]*0.9, f'Mean: {word_counts.mean():.2f}')\nplt.tight_layout()\nplt.savefig('case_length_distribution.png')\nprint(\"Histogram of case description length saved as 'case_length_distribution.png'\")\n\nprint(\"\\nDataset analysis complete.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-09T05:29:42.388214Z","iopub.execute_input":"2025-05-09T05:29:42.388533Z","iopub.status.idle":"2025-05-09T05:29:43.546230Z","shell.execute_reply.started":"2025-05-09T05:29:42.388510Z","shell.execute_reply":"2025-05-09T05:29:43.545258Z"}},"outputs":[{"name":"stdout","text":"Loading dataset...\nDataset loaded with shape: (22495, 2)\n\nColumns in dataset:\n['Case_Description', 'IPC_section']\n\nData types:\nCase_Description    object\nIPC_section         object\ndtype: object\n\nMissing values per column:\nCase_Description    0\nIPC_section         0\ndtype: int64\n\nExtracting 'Case_Description' and 'IPC_section' columns...\nSuccessfully extracted relevant columns.\n\nChecking for empty or whitespace-only entries...\nEmpty Case_Description entries: 0\nEmpty IPC_section entries: 0\n\nIPC Section Distribution:\nNumber of unique IPC sections: 409\nTop 10 most common IPC sections:\nIPC_section\nIPC 511     55\nIPC 120     55\nIPC 121     55\nIPC 122     55\nIPC 123     55\nIPC 124     55\nIPC 124A    55\nIPC 125     55\nIPC 126     55\nIPC 127     55\nName: count, dtype: int64\nBottom 10 least common IPC sections:\nIPC_section\nIPC 157    55\nIPC 158    55\nIPC 144    55\nIPC 145    55\nIPC 146    55\nIPC 147    55\nIPC 148    55\nIPC 149    55\nIPC 150    55\nIPC 151    55\nName: count, dtype: int64\n\nSimulating label encoding...\nNumber of unique encoded labels: 409\nSample of encoded labels: [0 0 0 0 0 0 0 0 0 0]\nCorresponding IPC sections: ['IPC 120', 'IPC 120', 'IPC 120', 'IPC 120', 'IPC 120', 'IPC 120', 'IPC 120', 'IPC 120', 'IPC 120', 'IPC 120']\n\nCase Description Analysis:\nAverage case description length: 94.45 characters\nMinimum length: 41 characters\nMaximum length: 216 characters\nAverage word count: 15.39 words\nMinimum word count: 6 words\nMaximum word count: 35 words\n\nIdentifying outliers in case description length...\nNumber of cases with extremely long descriptions (>99th percentile): 184\nSample outlier (truncated): In a village near Kolkata, a man was poisoned by his relatives over a property dispute. Forensic reports confirmed the poisoning, and all involved parties were arrested under IPC 302....\n\nChecking for special characters and non-ASCII characters...\nNumber of cases with non-ASCII characters: 1697\n\n====== SUMMARY OF FINDINGS ======\nTotal number of samples: 22495\nNumber of unique IPC sections: 409\nAverage case description length: 94.45 characters (15.39 words)\nMost common IPC section: IPC 511 with 55 occurrences\nLeast common IPC section: IPC 151 with 55 occurrences\nIPC section distribution balance: Balanced\n===================================\nPlot of top 20 IPC sections saved as 'ipc_distribution_top20.png'\nHistogram of case description length saved as 'case_length_distribution.png'\n\nDataset analysis complete.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1500x800 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x600 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install evaluate\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T17:29:54.069477Z","iopub.execute_input":"2025-04-04T17:29:54.069818Z","iopub.status.idle":"2025-04-04T17:30:00.802569Z","shell.execute_reply.started":"2025-04-04T17:29:54.069785Z","shell.execute_reply":"2025-04-04T17:30:00.801212Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport os\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom transformers import DataCollatorWithPadding\nfrom datasets import Dataset\nimport evaluate\nimport joblib\nfrom tqdm.auto import tqdm\n\n# Set random seeds for reproducibility\nseed = 42\ntorch.manual_seed(seed)\nnp.random.seed(seed)\n\n# Create necessary directories\nos.makedirs(\"./results\", exist_ok=True)\nos.makedirs(\"./logs\", exist_ok=True)\nos.makedirs(\"./tinybert_ipc_model\", exist_ok=True)\n\nprint(\"Starting IPC section classification training...\")\n\n# 1. Load and prepare dataset\nprint(\"Loading dataset...\")\ndf = pd.read_csv(\"/kaggle/input/ipc-section/Balanced_IPC_Sections_409_Cleaned.csv\")\ndf = df[['Case_Description', 'IPC_section']]\n\n# 2. Encode IPC section labels\nprint(\"Encoding labels...\")\nlabel_encoder = LabelEncoder()\ndf['label'] = label_encoder.fit_transform(df['IPC_section'])\nnum_labels = len(label_encoder.classes_)\nprint(f\"Number of unique IPC sections: {num_labels}\")\n\n# 3. Split into train and test sets\nprint(\"Splitting dataset...\")\ntrain_df, test_df = train_test_split(df, test_size=0.1, random_state=seed, stratify=df['label'])\nprint(f\"Training samples: {len(train_df)}, Test samples: {len(test_df)}\")\n\n# 4. Convert to Hugging Face Datasets\ntrain_dataset = Dataset.from_pandas(train_df)\ntest_dataset = Dataset.from_pandas(test_df)\n\n# 5. Initialize tokenizer\nprint(\"Loading tokenizer...\")\nmodel_checkpoint = \"prajjwal1/bert-tiny\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n\n# Since texts are short (avg 15 words), we can use a relatively short max_length\nmax_length = 128\n\n# 6. Preprocessing function\ndef preprocess_function(examples):\n    return tokenizer(\n        examples['Case_Description'], \n        truncation=True,\n        padding=\"max_length\",\n        max_length=max_length\n    )\n\n# 7. Apply preprocessing\nprint(\"Preprocessing datasets...\")\ntrain_dataset = train_dataset.map(preprocess_function, batched=True, batch_size=64)\ntest_dataset = test_dataset.map(preprocess_function, batched=True, batch_size=64)\n\n# Configure dataset format for PyTorch\ncolumns_to_return = ['input_ids', 'attention_mask', 'label']\ntrain_dataset.set_format(type='torch', columns=columns_to_return)\ntest_dataset.set_format(type='torch', columns=columns_to_return)\n\n# 8. Initialize model\nprint(\"Loading model...\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_checkpoint,\n    num_labels=num_labels\n)\n\n# 9. Define metrics\naccuracy = evaluate.load(\"accuracy\")\nf1 = evaluate.load(\"f1\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    \n    acc = accuracy.compute(predictions=predictions, references=labels)\n    # Use macro averaging since we have many classes\n    f1_macro = f1.compute(predictions=predictions, references=labels, average=\"macro\")\n    \n    return {\n        \"accuracy\": acc[\"accuracy\"],\n        \"f1_macro\": f1_macro[\"f1\"]\n    }\n\n# 10. Configure training arguments\nbatch_size = 32  # Increase batch size since texts are short\nprint(f\"Setting up training with batch size {batch_size}...\")\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=3e-5,  # Slightly higher learning rate\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size * 2,\n    num_train_epochs=50,  # More epochs for better learning\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    logging_steps=100,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1_macro\",  # Optimize for F1 macro since it's a multi-class problem\n    greater_is_better=True,\n    report_to=\"none\",  # Disable integrations for simplicity\n)\n\n# 11. Initialize Trainer\nprint(\"Initializing trainer...\")\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    tokenizer=None,  # We don't need to pass tokenizer since we already preprocessed\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\n# 12. Train model\nprint(\"Starting training...\")\ntrain_result = trainer.train()\nprint(f\"Training completed. Training metrics: {train_result.metrics}\")\n\n# 13. Evaluate model\nprint(\"Evaluating model...\")\neval_results = trainer.evaluate()\nprint(f\"Evaluation results: {eval_results}\")\n\n# 14. Save model, tokenizer, and label encoder\nprint(\"Saving model...\")\nmodel_path = \"./tinybert_ipc_model\"\ntrainer.save_model(model_path)\ntokenizer.save_pretrained(model_path)\njoblib.dump(label_encoder, os.path.join(model_path, \"label_encoder.pkl\"))\n\n# 15. Sample predictions\nprint(\"\\nTesting model with sample predictions...\")\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\n\ndef predict_ipc_section(text):\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=max_length)\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    \n    with torch.no_grad():\n        outputs = model(**inputs)\n    \n    logits = outputs.logits\n    predicted_class_id = logits.argmax(dim=-1).item()\n    predicted_label = label_encoder.inverse_transform([predicted_class_id])[0]\n    \n    # Calculate confidence\n    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n    confidence = probabilities[0][predicted_class_id].item()\n    \n    return predicted_label, confidence\n\n# Test a few samples\nsample_texts = test_df['Case_Description'].iloc[:3].tolist()\nfor i, text in enumerate(sample_texts):\n    pred_label, confidence = predict_ipc_section(text)\n    true_label = test_df['IPC_section'].iloc[i]\n    print(f\"\\nSample {i+1}:\")\n    print(f\"Text: {text}\")\n    print(f\"True IPC section: {true_label}\")\n    print(f\"Predicted IPC section: {pred_label}\")\n    print(f\"Confidence: {confidence:.4f}\")\n\nprint(\"\\nTraining and evaluation complete!\")\nprint(f\"Model saved to {model_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T17:30:11.965829Z","iopub.execute_input":"2025-04-04T17:30:11.966132Z","iopub.status.idle":"2025-04-04T17:38:23.263351Z","shell.execute_reply.started":"2025-04-04T17:30:11.966108Z","shell.execute_reply":"2025-04-04T17:38:23.262631Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom transformers import DataCollatorWithPadding\nimport joblib\nimport torch\nimport os\nimport evaluate\nimport numpy as np\n\n# Load previously saved model and tokenizer\nmodel_path = \"./tinybert_ipc_model\"\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nlabel_encoder = joblib.load(os.path.join(model_path, \"label_encoder.pkl\"))\nnum_labels = len(label_encoder.classes_)\n\n# Send model to GPU if available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\n\n# Define metrics again\naccuracy = evaluate.load(\"accuracy\")\nf1 = evaluate.load(\"f1\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    acc = accuracy.compute(predictions=predictions, references=labels)\n    f1_macro = f1.compute(predictions=predictions, references=labels, average=\"macro\")\n    return {\"accuracy\": acc[\"accuracy\"], \"f1_macro\": f1_macro[\"f1\"]}\n\n# Reuse tokenized and formatted datasets from previous code\n# (train_dataset and test_dataset must still be in memory)\n# If not, reload and tokenize again using your original logic\n\n# Define new training arguments for additional epochs\ntraining_args = TrainingArguments(\n    output_dir=\"./results_continue\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=3e-5,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=64,\n    num_train_epochs=100,  #  Continue for 100 more epochs\n    weight_decay=0.01,\n    logging_dir=\"./logs_continue\",\n    logging_steps=100,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1_macro\",\n    greater_is_better=True,\n    resume_from_checkpoint=model_path,\n    report_to=\"none\"\n)\n\n# Re-initialize Trainer\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,  #  reuse your in-memory datasets\n    eval_dataset=test_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\n# Continue training\nprint(\" Continuing fine-tuning for 100 more epochs...\")\ntrainer.train()\n\n# Save the continued model\ncontinued_path = \"./tinybert_ipc_model_continued\"\ntrainer.save_model(continued_path)\ntokenizer.save_pretrained(continued_path)\njoblib.dump(label_encoder, os.path.join(continued_path, \"label_encoder.pkl\"))\n\nprint(f\" Fine-tuning complete. New model saved to {continued_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T17:41:43.523319Z","iopub.execute_input":"2025-04-04T17:41:43.523779Z","iopub.status.idle":"2025-04-04T17:57:48.997136Z","shell.execute_reply.started":"2025-04-04T17:41:43.523747Z","shell.execute_reply":"2025-04-04T17:57:48.996326Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nimport torch\nimport numpy as np\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom datasets import Dataset\n\n# Load Pretrained Model (e.g., LegalBERT)\nmodel_path = \"nlpaueb/legal-bert\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=len(label_encoder.classes_), problem_type=\"multi_label_classification\")\n\n# Load your dataset (assuming multi-label format)\ntrain_dataset = Dataset.from_pandas(train_df)\ntest_dataset = Dataset.from_pandas(test_df)\n\n# Tokenize text with context\ndef preprocess_function(examples):\n    return tokenizer(examples['Case_Description'], truncation=True, padding=True, max_length=128)\n\ntrain_dataset = train_dataset.map(preprocess_function, batched=True)\ntest_dataset = test_dataset.map(preprocess_function, batched=True)\n\n# Convert labels to multi-hot encoding\nmlb = MultiLabelBinarizer()\ntrain_labels = mlb.fit_transform(train_df['IPC_section'].apply(lambda x: x.split(',')))  # Assuming multiple sections are comma-separated\ntest_labels = mlb.transform(test_df['IPC_section'].apply(lambda x: x.split(',')))\n\n# Update the dataset to include multi-hot encoded labels\ntrain_dataset = train_dataset.add_column(\"labels\", train_labels)\ntest_dataset = test_dataset.add_column(\"labels\", test_labels)\n\n# Training Arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results_continue\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=3e-5,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=64,\n    num_train_epochs=50,\n    weight_decay=0.01,\n    logging_dir=\"./logs_continue\",\n    logging_steps=100,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1_macro\",\n    greater_is_better=True,\n    report_to=\"none\"\n)\n\n# Trainer Initialization\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\n\n# Fine-Tuning Model\nprint(\" Continuing fine-tuning for 100 more epochs...\")\ntrainer.train()\n\n# Save Continued Model\ntrainer.save_model(\"./tinybert_ipc_model_continued_context\")\ntokenizer.save_pretrained(\"./tinybert_ipc_model_continued_context\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T18:07:52.767486Z","iopub.execute_input":"2025-04-04T18:07:52.767826Z","iopub.status.idle":"2025-04-04T18:07:52.862346Z","shell.execute_reply.started":"2025-04-04T18:07:52.767798Z","shell.execute_reply":"2025-04-04T18:07:52.861228Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_ipc_section(text):\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=max_length)\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    \n    with torch.no_grad():\n        outputs = model(**inputs)\n    \n    logits = outputs.logits\n    predicted_class_id = logits.argmax(dim=-1).item()\n    predicted_label = label_encoder.inverse_transform([predicted_class_id])[0]\n    \n    # Calculate confidence\n    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n    confidence = probabilities[0][predicted_class_id].item()\n    \n    return predicted_label, confidence\n\n# Test a few samples\nsample_texts = test_df['Case_Description'].iloc[:3].tolist()\nfor i, text in enumerate(sample_texts):\n    pred_label, confidence = predict_ipc_section(text)\n    true_label = test_df['IPC_section'].iloc[i]\n    print(f\"\\nSample {i+1}:\")\n    print(f\"Text: {text}\")\n    print(f\"True IPC section: {true_label}\")\n    print(f\"Predicted IPC section: {pred_label}\")\n    print(f\"Confidence: {confidence:.4f}\")\n\nprint(\"\\nTraining and evaluation complete!\")\nprint(f\"Model saved to {model_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T17:59:58.567999Z","iopub.execute_input":"2025-04-04T17:59:58.568384Z","iopub.status.idle":"2025-04-04T17:59:58.589086Z","shell.execute_reply.started":"2025-04-04T17:59:58.568339Z","shell.execute_reply":"2025-04-04T17:59:58.588234Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_texts = [\n    \"The accused forcibly entered the victim's house at night and stole cash and jewelry.\",\n    \"A man was found driving a stolen motorcycle and failed to produce ownership documents.\",\n    \"The complainant alleges that her husband and in-laws have been harassing her for dowry.\",\n    \"An individual was caught cheating during a government recruitment exam using electronic devices.\",\n    \"The victim was attacked with a sharp weapon during a dispute over property boundaries.\",\n    \"The company director siphoned off investor money by falsifying financial statements.\",\n    \"An individual created a fake Facebook profile to defame a college professor.\",\n    \"A gang threatened a shopkeeper with firearms and demanded protection money.\",\n    \"Two men were found in possession of illegal arms and explosives during a late-night patrol.\",\n    \"The accused assaulted a traffic policeman who tried to stop him from jumping a red light.\",\n    \"A student hacked college profile to change his university marks\",\n    \"A hacker gained unauthorized access to the companys database, altering confidential customer information and stealingsensitivedata\",\n    \"Public disturbance casued by students singing songs\",\n    \"A man was arrested for making false bomb threats\",\n    \"Woman was gang raped by five men\"\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T18:00:08.804181Z","iopub.execute_input":"2025-04-04T18:00:08.804537Z","iopub.status.idle":"2025-04-04T18:00:08.809015Z","shell.execute_reply.started":"2025-04-04T18:00:08.804507Z","shell.execute_reply":"2025-04-04T18:00:08.808127Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n Testing 10 manually created legal case descriptions...\\n\")\n\nfor i, text in enumerate(sample_texts):\n    pred_label, confidence = predict_ipc_section(text)\n    print(f\" Sample {i+1}:\")\n    print(f\" Text: {text}\")\n    print(f\" Predicted IPC Section: {pred_label}\")\n    print(f\" Confidence: {confidence:.4f}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T18:00:12.321650Z","iopub.execute_input":"2025-04-04T18:00:12.321928Z","iopub.status.idle":"2025-04-04T18:00:12.377355Z","shell.execute_reply.started":"2025-04-04T18:00:12.321908Z","shell.execute_reply":"2025-04-04T18:00:12.376677Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom transformers import DataCollatorWithPadding\nimport joblib\nimport torch\nimport os\nimport evaluate\nimport numpy as np\n\n# Load previously saved model and tokenizer\nmodel_path = \"./tinybert_ipc_model_continued\"\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nlabel_encoder = joblib.load(os.path.join(model_path, \"label_encoder.pkl\"))\nnum_labels = len(label_encoder.classes_)\n\n# Send model to GPU if available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\n\n# Define metrics again\naccuracy = evaluate.load(\"accuracy\")\nf1 = evaluate.load(\"f1\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    acc = accuracy.compute(predictions=predictions, references=labels)\n    f1_macro = f1.compute(predictions=predictions, references=labels, average=\"macro\")\n    return {\"accuracy\": acc[\"accuracy\"], \"f1_macro\": f1_macro[\"f1\"]}\n\n# Reuse tokenized and formatted datasets from previous code\n# (train_dataset and test_dataset must still be in memory)\n# If not, reload and tokenize again using your original logic\n\n# Define new training arguments for additional epochs\ntraining_args = TrainingArguments(\n    output_dir=\"./results_continue\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=3e-5,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=64,\n    num_train_epochs=100,  #  Continue for 100 more epochs\n    weight_decay=0.01,\n    logging_dir=\"./logs_continue\",\n    logging_steps=100,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1_macro\",\n    greater_is_better=True,\n    resume_from_checkpoint=model_path,\n    report_to=\"none\"\n)\n\n# Re-initialize Trainer\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,  #  reuse your in-memory datasets\n    eval_dataset=test_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\n# Continue training\nprint(\" Continuing fine-tuning for 50 more epochs...\")\ntrainer.train()\n\n# Save the continued model\ncontinued_path = \"./tinybert_ipc_model_continued_1\"\ntrainer.save_model(continued_path)\ntokenizer.save_pretrained(continued_path)\njoblib.dump(label_encoder, os.path.join(continued_path, \"label_encoder.pkl\"))\n\nprint(f\" Fine-tuning complete. New model saved to {continued_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T18:19:08.035178Z","iopub.execute_input":"2025-04-04T18:19:08.035517Z","iopub.status.idle":"2025-04-04T18:35:20.200011Z","shell.execute_reply.started":"2025-04-04T18:19:08.035489Z","shell.execute_reply":"2025-04-04T18:35:20.199279Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport joblib\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n# Path to your continued model\nmodel_path = \"/kaggle/working/tinybert_ipc_model_continued_1\"\nmax_length = 128\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n#  Load model, tokenizer, label encoder\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)\nlabel_encoder = joblib.load(f\"{model_path}/label_encoder.pkl\")\n\n#  Prediction function\ndef predict_ipc_section(text):\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=max_length)\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n    \n    logits = outputs.logits\n    predicted_class_id = logits.argmax(dim=-1).item()\n    predicted_label = label_encoder.inverse_transform([predicted_class_id])[0]\n\n    # Confidence score\n    probs = torch.nn.functional.softmax(logits, dim=-1)\n    confidence = probs[0][predicted_class_id].item()\n    \n    return predicted_label, confidence\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T06:19:22.086169Z","iopub.execute_input":"2025-04-05T06:19:22.086474Z","iopub.status.idle":"2025-04-05T06:19:22.210201Z","shell.execute_reply.started":"2025-04-05T06:19:22.086452Z","shell.execute_reply":"2025-04-05T06:19:22.208970Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/kaggle/working/tinybert_ipc_model_continued'. Use `repo_type` argument if needed.","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-874ff0308efb>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#  Load model, tokenizer, label encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mlabel_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{model_path}/label_encoder.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;31m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m         \u001b[0mtokenizer_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenizer_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    859\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_commit_hash\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[0mcommit_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m     resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    691\u001b[0m         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0mTOKENIZER_CONFIG_FILE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"There was a specific connection error when trying to load {path_or_repo_id}:\\n{err}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHFValidationError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    470\u001b[0m             \u001b[0;34mf\"Incorrect path_or_model_id: '{path_or_repo_id}'. Please provide either the path to a local folder or the repo_id of a model on the Hub.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         ) from e\n","\u001b[0;31mOSError\u001b[0m: Incorrect path_or_model_id: '/kaggle/working/tinybert_ipc_model_continued'. Please provide either the path to a local folder or the repo_id of a model on the Hub."],"ename":"OSError","evalue":"Incorrect path_or_model_id: '/kaggle/working/tinybert_ipc_model_continued'. Please provide either the path to a local folder or the repo_id of a model on the Hub.","output_type":"error"}],"execution_count":7},{"cell_type":"code","source":"# Sample text\ntext = \"A man was arrested for making false bomb threats\"\n\n# Predict\npredicted_ipc, confidence = predict_ipc_section(text)\n\nprint(\" Predicted IPC Section:\", predicted_ipc)\nprint(\" Confidence Score:\", round(confidence, 4))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T18:35:47.921284Z","iopub.execute_input":"2025-04-04T18:35:47.921586Z","iopub.status.idle":"2025-04-04T18:35:47.930600Z","shell.execute_reply.started":"2025-04-04T18:35:47.921563Z","shell.execute_reply":"2025-04-04T18:35:47.929834Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_texts = [\n    \"The accused stabbed the victim during a family dispute.\",\n    \"Woman was raped and killed by a man without her consent\",\n    \"The woman was attacked by a neighbour\",\n    \"car drove at high speed and killed two pedestrians.\",\n    \"The man posted obscene photos of his ex on social media.\",\n    \"A gang robbed a bank and injured the guard.\",\n    \"Man slapped a police officer on duty\",\n    \"A fake job offer was used to cheat young graduates.\",\n    \"Students damaged public property during riot.\",\n    \"A man molested a 14-year-old girl in a market.\",\n    \"A group of thieves robbed a bank using guns and hostages.\"\n]\n\nfor i, text in enumerate(sample_texts):\n    label, conf = predict_ipc_section(text)\n    print(f\"\\n Sample {i+1}\")\n    print(f\" Text: {text}\")\n    print(f\" Predicted IPC Section: {label}\")\n    print(f\" Confidence: {conf:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T18:48:59.839790Z","iopub.execute_input":"2025-04-04T18:48:59.840092Z","iopub.status.idle":"2025-04-04T18:48:59.881015Z","shell.execute_reply.started":"2025-04-04T18:48:59.840071Z","shell.execute_reply":"2025-04-04T18:48:59.880412Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"RUN THE BELOW CODE ONLY","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport joblib\nimport numpy as np\n\n# Load the previously saved model, tokenizer, and label encoder\nmodel_path = \"/kaggle/input/ipc-prediction/tensorflow2/default/1\"\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path, local_files_only=True)\ntokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\nlabel_encoder = joblib.load(os.path.join(model_path, \"label_encoder.pkl\"))\n\n# Send model to GPU if available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\n\n# Define inference function\ndef predict_ipc_section(text):\n    # Tokenize the input text\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    \n    # Make prediction\n    with torch.no_grad():\n        outputs = model(**inputs)\n    \n    # Get the predicted class ID\n    logits = outputs.logits\n    predicted_class_id = logits.argmax(dim=-1).item()\n    \n    # Get the predicted label (IPC section)\n    predicted_label = label_encoder.inverse_transform([predicted_class_id])[0]\n    \n    # Calculate confidence\n    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n    confidence = probabilities[0][predicted_class_id].item()\n    \n    return predicted_label, confidence\n\n# Example: Testing the model with a few case descriptions\nsample_texts = [\n    \"The accused stabbed the victim during a family dispute.\",\n    \"Woman was raped and killed by a man without her consent.\",\n    \"The woman was attacked by a neighbour.\",\n    \"A guy was shot by gang of thieves\",\n    \"The man posted obscene photos of his ex on social media.\",\n    \"A gang robbed a bank and injured the guard.\",\n    \"A man molested a 14-year-old girl in a market.\",\n    \"A group of thieves robbed a bank using guns and hostages\",\n]\n\n# Test the model on new examples\nfor i, text in enumerate(sample_texts):\n    pred_label, confidence = predict_ipc_section(text)\n    print(f\"\\nSample {i+1}:\")\n    print(f\"Text: {text}\")\n    print(f\"Predicted IPC section: {pred_label}\")\n    print(f\"Confidence: {confidence:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T05:30:32.955572Z","iopub.execute_input":"2025-05-09T05:30:32.955874Z","iopub.status.idle":"2025-05-09T05:30:33.060499Z","shell.execute_reply.started":"2025-05-09T05:30:32.955853Z","shell.execute_reply":"2025-05-09T05:30:33.059737Z"}},"outputs":[{"name":"stdout","text":"\nSample 1:\nText: The accused stabbed the victim during a family dispute.\nPredicted IPC section: IPC 320\nConfidence: 0.5829\n\nSample 2:\nText: Woman was raped and killed by a man without her consent.\nPredicted IPC section: IPC 375\nConfidence: 0.9789\n\nSample 3:\nText: The woman was attacked by a neighbour.\nPredicted IPC section: IPC 333\nConfidence: 0.8012\n\nSample 4:\nText: A guy was shot by gang of thieves\nPredicted IPC section: IPC 396\nConfidence: 0.1445\n\nSample 5:\nText: The man posted obscene photos of his ex on social media.\nPredicted IPC section: IPC 294\nConfidence: 0.5188\n\nSample 6:\nText: A gang robbed a bank and injured the guard.\nPredicted IPC section: IPC 390\nConfidence: 0.6313\n\nSample 7:\nText: A man molested a 14-year-old girl in a market.\nPredicted IPC section: IPC 354\nConfidence: 0.8230\n\nSample 8:\nText: A group of thieves robbed a bank using guns and hostages\nPredicted IPC section: IPC 391\nConfidence: 0.5709\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport joblib\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n\n# Load model, tokenizer, label encoder\nmodel_path = \"/kaggle/working/tinybert_ipc_model_continued_1\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmax_length = 128\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nlabel_encoder = joblib.load(f\"{model_path}/label_encoder.pkl\")\n\n# Prediction function\ndef predict_ipc_section(text):\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=max_length)\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n    \n    logits = outputs.logits\n    predicted_class_id = torch.argmax(logits, dim=1).item()\n    predicted_label = label_encoder.inverse_transform([predicted_class_id])[0]\n    return predicted_label\n\n# 50 Manually Written Test Samples with Expected IPC Sections\nsamples = [\n    (\"A man stabbed his neighbor during an argument.\", \"IPC 307\"),\n    (\"A girl was sexually assaulted in a public park.\", \"IPC 354\"),\n    (\"The accused broke into a house at night and stole jewelry.\", \"IPC 457\"),\n    (\"A fake doctor was found practicing medicine without a license.\", \"IPC 419\"),\n    (\"A person created a fake PAN card to get a bank loan.\", \"IPC 468\"),\n    (\"A group looted a petrol pump with firearms.\", \"IPC 395\"),\n    (\"A woman was set on fire by her in-laws for dowry.\", \"IPC 304B\"),\n    (\"A man intentionally set fire to a crop field.\", \"IPC 435\"),\n    (\"A factory released toxic gas, causing public harm.\", \"IPC 269\"),\n    (\"A man threatened to leak private pictures of his ex-girlfriend.\", \"IPC 506\"),\n    (\"A mob vandalized government buses during a protest.\", \"IPC 427\"),\n    (\"A shopkeeper sold expired medicines knowingly.\", \"IPC 273\"),\n    (\"A man pretended to be a police officer to collect bribes.\", \"IPC 170\"),\n    (\"A man made hoax bomb calls to a railway station.\", \"IPC 505\"),\n    (\"A minor girl was raped and murdered in a hostel.\", \"IPC 376A\"),\n    (\"A man slapped a police officer on duty.\", \"IPC 186\"),\n    (\"A husband abandoned his wife without any reason.\", \"IPC 498\"),\n    (\"A person was caught spying for a foreign country.\", \"IPC 121\"),\n    (\"A student leaked the board exam paper for money.\", \"IPC 420\"),\n    (\"A bank manager misappropriated public funds.\", \"IPC 409\"),\n    (\"A man took money for a job but never delivered.\", \"IPC 417\"),\n    (\"A factory employee damaged a machine during a strike.\", \"IPC 427\"),\n    (\"A driver hit a pedestrian and fled the scene.\", \"IPC 304A\"),\n    (\"A man kept obscene books for sale.\", \"IPC 292\"),\n    (\"A hacker defaced a government website.\", \"IPC 465\"),\n    (\"A boy molested a classmate during a school trip.\", \"IPC 354\"),\n    (\"A woman killed her abusive husband in self-defense.\", \"IPC 100\"),\n    (\"A politician gave a hate speech against a religious group.\", \"IPC 153A\"),\n    (\"A contractor used low-grade material in a public bridge.\", \"IPC 406\"),\n    (\"A man sold fake COVID vaccines online.\", \"IPC 420\"),\n    (\"A group forced shops to shut down during a bandh.\", \"IPC 143\"),\n    (\"A man kidnapped his ex-girlfriend for marriage.\", \"IPC 366\"),\n    (\"A teenager was caught with illegal firearms.\", \"IPC 3\"),\n    (\"A WhatsApp message caused panic about a child kidnapper gang.\", \"IPC 505\"),\n    (\"A woman was harassed at her workplace with obscene messages.\", \"IPC 509\"),\n    (\"A customer beat a delivery boy over late delivery.\", \"IPC 323\"),\n    (\"A girls private photos were leaked by a classmate.\", \"IPC 354C\"),\n    (\"A truck carrying stolen electronics was intercepted.\", \"IPC 411\"),\n    (\"A false FIR was filed against a neighbor out of revenge.\", \"IPC 182\"),\n    (\"A man spit on a cop during COVID lockdown.\", \"IPC 270\"),\n    (\"A railway employee issued fake train tickets.\", \"IPC 420\"),\n    (\"A man forcibly married a widow against her will.\", \"IPC 366\"),\n    (\"A teacher beat a student causing serious injury.\", \"IPC 325\"),\n    (\"A group scammed people in the name of fake astrology services.\", \"IPC 420\"),\n    (\"A woman lied under oath in court.\", \"IPC 193\"),\n    (\"A man threw acid on a woman for rejecting his proposal.\", \"IPC 326A\"),\n    (\"A student blackmailed a professor using fake screenshots.\", \"IPC 384\"),\n    (\"A cab driver tried to grope a woman passenger.\", \"IPC 354A\"),\n    (\"A person refused to return loan money and threatened the lender.\", \"IPC 406\"),\n    (\"A school principal siphoned funds meant for student meals.\", \"IPC 409\"),\n]\n\n# Run predictions and compare\ncorrect = 0\nprint(\"\\n Evaluating 50 manual samples:\\n\")\n\nfor i, (text, true_label) in enumerate(samples):\n    pred_label = predict_ipc_section(text)\n    status = \"\" if pred_label == true_label else \"\"\n    if status == \"\":\n        correct += 1\n    print(f\"{status} Sample {i+1}:\")\n    print(f\" Text: {text}\")\n    print(f\" True: {true_label}\")\n    print(f\" Pred: {pred_label}\\n\")\n\n# Accuracy\naccuracy = correct / len(samples)\nprint(f\" Accuracy on 50 manual samples: {accuracy:.4f} ({correct}/50 correct)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T07:31:21.856081Z","iopub.execute_input":"2025-04-04T07:31:21.856394Z","iopub.status.idle":"2025-04-04T07:31:22.083849Z","shell.execute_reply.started":"2025-04-04T07:31:21.856371Z","shell.execute_reply":"2025-04-04T07:31:22.082999Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_cases = [\n    \"A man broke into a house at midnight and stole expensive electronics.\",\n    \"A teenager was caught red-handed while trying to pickpocket at a railway station.\",\n    \"A woman accused her husband and in-laws of harassing her for dowry.\",\n    \"A group of masked men looted a jewelry store at gunpoint.\",\n    \"A school teacher was found guilty of physically assaulting a student.\",\n    \"A man was arrested for forging signatures to withdraw money from a bank account.\",\n    \"Someone created a fake government website to scam people into paying for services.\",\n    \"A car accident occurred due to drunk driving, resulting in the death of a pedestrian.\",\n    \"A woman was followed and touched inappropriately by a stranger on a bus.\",\n    \"A student hacked into the university system and changed his grades.\",\n    \"An employee was found leaking confidential data to a rival company.\",\n    \"A man was caught making fake identity cards for illegal migrants.\",\n    \"A person circulated false rumors on WhatsApp that caused a public riot.\",\n    \"A man was found filming women in a shopping mall's trial rooms.\",\n    \"A husband abandoned his wife and two children without any financial support.\",\n    \"A cybercriminal used phishing emails to steal credit card details.\",\n    \"A customer punched a delivery agent for delivering the wrong package.\",\n    \"A shopkeeper was attacked for refusing to pay weekly protection money.\",\n    \"A minor girl was lured with gifts and later assaulted by her neighbor.\",\n    \"A man threatened to upload private photos of his ex-girlfriend unless she paid him.\"\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T07:35:02.887154Z","iopub.execute_input":"2025-04-04T07:35:02.887459Z","iopub.status.idle":"2025-04-04T07:35:02.891573Z","shell.execute_reply.started":"2025-04-04T07:35:02.887439Z","shell.execute_reply":"2025-04-04T07:35:02.890550Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_ipc_section(text):\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=max_length)\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    \n    with torch.no_grad():\n        outputs = model(**inputs)\n    \n    logits = outputs.logits\n    predicted_class_id = logits.argmax(dim=-1).item()\n    predicted_label = label_encoder.inverse_transform([predicted_class_id])[0]\n    \n    probs = torch.nn.functional.softmax(logits, dim=-1)\n    confidence = probs[0][predicted_class_id].item()\n    \n    return predicted_label, confidence  #  Return both\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T07:35:42.507277Z","iopub.execute_input":"2025-04-04T07:35:42.507556Z","iopub.status.idle":"2025-04-04T07:35:42.512878Z","shell.execute_reply.started":"2025-04-04T07:35:42.507535Z","shell.execute_reply":"2025-04-04T07:35:42.512156Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i, text in enumerate(test_cases):\n    pred_label, confidence = predict_ipc_section(text)\n    print(f\"\\n Sample {i+1}:\")\n    print(f\" Text: {text}\")\n    print(f\" Predicted IPC Section: {pred_label}\")\n    print(f\" Confidence: {confidence:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T07:35:49.455362Z","iopub.execute_input":"2025-04-04T07:35:49.455650Z","iopub.status.idle":"2025-04-04T07:35:49.536450Z","shell.execute_reply.started":"2025-04-04T07:35:49.455628Z","shell.execute_reply":"2025-04-04T07:35:49.535424Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import spacy\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom transformers import DataCollatorWithPadding\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom datasets import Dataset\nimport joblib\nimport os\nfrom torch import nn\nfrom transformers import BertConfig\n\n# Load spaCy's model for NER and Dependency Parsing\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Load your dataset\ndf = pd.read_csv(\"/kaggle/input/ipc-section/Balanced_IPC_Sections_409_Cleaned.csv\")\ndf = df[['Case_Description', 'IPC_section']]\n\n# Initialize tokenizer - using TinyBERT vocabulary but will train from scratch\nmodel_name = \"prajjwal1/bert-tiny\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Named Entity Recognition (NER) & Dependency Parsing with legal context awareness\ndef extract_legal_features(text):\n    doc = nlp(text)\n    \n    # Extract legal entities (focusing on relevant ones)\n    legal_entities = []\n    for ent in doc.ents:\n        if ent.label_ in [\"PERSON\", \"ORG\", \"GPE\", \"DATE\", \"LAW\", \"MONEY\"]:\n            legal_entities.append(f\"{ent.label_}:{ent.text}\")\n    \n    # Extract key dependencies for legal context\n    legal_dependencies = []\n    for token in doc:\n        # Focus on subject-verb-object relationships and other important dependencies\n        if token.dep_ in [\"nsubj\", \"dobj\", \"prep\", \"pobj\", \"ROOT\", \"amod\"]:\n            if token.head.pos_ in [\"VERB\", \"NOUN\"]:\n                legal_dependencies.append(f\"{token.dep_}:{token.text}-{token.head.text}\")\n    \n    # Extract key legal phrases (n-grams focused on legal terminology)\n    legal_phrases = []\n    for chunk in doc.noun_chunks:\n        if any(legal_term in chunk.text.lower() for legal_term in \n               [\"law\", \"court\", \"plaintiff\", \"defendant\", \"section\", \"act\", \"offense\", \"crime\", \"case\"]):\n            legal_phrases.append(f\"LEGAL_PHRASE:{chunk.text}\")\n    \n    # Combine all features with appropriate markers\n    legal_features = \" \".join(legal_entities + legal_dependencies + legal_phrases)\n    return legal_features\n\n# Apply feature extraction to each case description\ndef preprocess_function(examples):\n    case_desc_batch = examples['Case_Description']\n    \n    # Prepare enriched texts with legal context features\n    enriched_texts = []\n    for case_desc in case_desc_batch:\n        legal_features = extract_legal_features(case_desc)\n        # Combine original text with extracted legal features\n        enriched_text = case_desc + \" [LEGAL_CONTEXT] \" + legal_features\n        enriched_texts.append(enriched_text)\n    \n    # Tokenize with appropriate padding and truncation\n    return tokenizer(enriched_texts, truncation=True, padding='max_length', max_length=256)\n\n# Prepare multi-label format\ndf['IPC_section'] = df['IPC_section'].apply(lambda x: x.split(','))\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\nval_df, test_df = train_test_split(test_df, test_size=0.5, random_state=42)\n\n# Convert to Datasets\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\ntest_dataset = Dataset.from_pandas(test_df)\n\n# Apply preprocessing with legal context\ntrain_dataset = train_dataset.map(preprocess_function, batched=True)\nval_dataset = val_dataset.map(preprocess_function, batched=True)\ntest_dataset = test_dataset.map(preprocess_function, batched=True)\n\n# Prepare multi-label binarization\nmlb = MultiLabelBinarizer()\ntrain_labels = mlb.fit_transform(train_df['IPC_section'])\nval_labels = mlb.transform(val_df['IPC_section'])\ntest_labels = mlb.transform(test_df['IPC_section'])\n\n# Convert to list format\ntrain_dataset = train_dataset.add_column(\"labels\", [list(label) for label in train_labels])\nval_dataset = val_dataset.add_column(\"labels\", [list(label) for label in val_labels])\ntest_dataset = test_dataset.add_column(\"labels\", [list(label) for label in test_labels])\n\n# Create a new model configuration from scratch\nconfig = BertConfig.from_pretrained(model_name)\nconfig.num_labels = len(mlb.classes_)\nconfig.problem_type = \"multi_label_classification\"\nconfig.hidden_dropout_prob = 0.2  # Increase dropout for better generalization\nconfig.attention_probs_dropout_prob = 0.2\n\n# Initialize model from scratch with this configuration\nmodel = AutoModelForSequenceClassification.from_config(config)\n\n# Custom loss function for multi-label classification with focal loss\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=2):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n        \n    def forward(self, inputs, targets):\n        BCE_loss = self.bce(inputs, targets)\n        pt = torch.exp(-BCE_loss)\n        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n        return F_loss.mean()\n\n# Compute evaluation metrics\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    # Apply sigmoid to get probabilities\n    sigmoid_preds = 1 / (1 + np.exp(-predictions))\n    # Apply threshold\n    predictions = (sigmoid_preds > 0.5).astype(int)\n    \n    # Calculate metrics\n    correct = (predictions == labels).sum()\n    total = labels.size\n    accuracy = correct / total\n    \n    # F1 score calculation\n    true_positives = (predictions * labels).sum()\n    precision = true_positives / predictions.sum() if predictions.sum() > 0 else 0\n    recall = true_positives / labels.sum() if labels.sum() > 0 else 0\n    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n    \n    return {\n        \"accuracy\": accuracy,\n        \"f1\": f1,\n        \"precision\": precision,\n        \"recall\": recall\n    }\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./ipc_tinybert_scratch\",\n    evaluation_strategy=\"steps\",\n    eval_steps=100,\n    save_strategy=\"steps\",\n    save_steps=100,\n    learning_rate=5e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=32,\n    num_train_epochs=10,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    logging_steps=50,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n    report_to=\"none\",\n    warmup_steps=500,\n    fp16=True,  # Enable mixed precision training\n    gradient_accumulation_steps=2\n)\n\n# Data collator with dynamic padding\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n# Custom trainer with focal loss\nclass CustomTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.pop(\"labels\").float()\n        outputs = model(**inputs)\n        logits = outputs.logits\n        \n        # Apply focal loss\n        loss_fct = FocalLoss(alpha=1, gamma=2)\n        loss = loss_fct(logits, labels)\n        \n        return (loss, outputs) if return_outputs else loss\n\n# Initialize trainer\ntrainer = CustomTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\n# Train the model\nprint(\"Training the TinyBERT model from scratch with legal context awareness...\")\ntrainer.train()\n\n# Evaluate on test set\nprint(\"Evaluating on test set...\")\ntest_results = trainer.evaluate(test_dataset)\nprint(f\"Test results: {test_results}\")\n\n# Save the model, tokenizer, and label encoder\nmodel_path = \"./ipc_legal_context_model\"\nmodel.save_pretrained(model_path)\ntokenizer.save_pretrained(model_path)\njoblib.dump(mlb, os.path.join(model_path, \"label_encoder.pkl\"))\n\nprint(\"Model training complete and saved.\")\n\n# Function to make predictions on new cases\ndef predict_ipc_sections(case_text, model, tokenizer, mlb):\n    # Extract legal features\n    legal_features = extract_legal_features(case_text)\n    enriched_text = case_text + \" [LEGAL_CONTEXT] \" + legal_features\n    \n    # Tokenize\n    inputs = tokenizer(enriched_text, truncation=True, padding='max_length', \n                       max_length=256, return_tensors=\"pt\")\n    \n    # Make prediction\n    model.eval()\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n        probs = torch.sigmoid(logits)\n        predictions = (probs > 0.5).int().cpu().numpy()\n    \n    # Convert to IPC sections\n    predicted_sections = mlb.inverse_transform(predictions)[0]\n    return predicted_sections, probs.cpu().numpy()[0]\n\n# Example of usage\nprint(\"\\nExample prediction:\")\nexample_case = \"The accused was charged with stealing a motorcycle from the complainant's house.\"\nmodel.to(\"cpu\")  # Move to CPU for inference\npredicted_sections, confidence_scores = predict_ipc_sections(example_case, model, tokenizer, mlb)\nprint(f\"Case: {example_case}\")\nprint(f\"Predicted IPC Sections: {predicted_sections}\")\nprint(f\"Confidence scores: {confidence_scores}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T13:16:07.151182Z","iopub.execute_input":"2025-04-04T13:16:07.151554Z","iopub.status.idle":"2025-04-04T13:21:16.940955Z","shell.execute_reply.started":"2025-04-04T13:16:07.151529Z","shell.execute_reply":"2025-04-04T13:21:16.939298Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Inspect the dataset structure\nimport pandas as pd\nfrom datasets import Dataset\nfrom sklearn.preprocessing import MultiLabelBinarizer\nimport torch\nfrom transformers import AutoTokenizer\n\n# Load your dataset\ndf = pd.read_csv(\"/kaggle/input/ipc-section/Balanced_IPC_Sections_409_Cleaned.csv\")\nprint(\"Original DataFrame shape:\", df.shape)\nprint(\"\\nColumns in DataFrame:\", df.columns.tolist())\nprint(\"\\nFirst 2 rows:\")\nprint(df.head(2))\n\n# Check IPC section format\nprint(\"\\nSample IPC sections (first 3 rows):\")\nprint(df['IPC_section'].head(3))\nprint(\"\\nIPC section data type:\", type(df['IPC_section'].iloc[0]))\n\n# Convert IPC sections to list format if they're strings\nif isinstance(df['IPC_section'].iloc[0], str):\n    print(\"\\nConverting string IPC sections to lists...\")\n    df['IPC_section'] = df['IPC_section'].apply(lambda x: x.split(','))\n\n# Check unique IPC sections\nall_sections = []\nfor sections in df['IPC_section']:\n    all_sections.extend(sections)\nunique_sections = set(all_sections)\nprint(\"\\nNumber of unique IPC sections:\", len(unique_sections))\nprint(\"Sample unique sections (first 5):\", list(unique_sections)[:5])\n\n# Create train/val/test split for inspection\nfrom sklearn.model_selection import train_test_split\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\nval_df, test_df = train_test_split(test_df, test_size=0.5, random_state=42)\n\nprint(\"\\nSplit sizes:\")\nprint(f\"Train: {len(train_df)}, Validation: {len(val_df)}, Test: {len(test_df)}\")\n\n# Create datasets\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\ntest_dataset = Dataset.from_pandas(test_df)\n\n# Show dataset features\nprint(\"\\nDataset features:\", train_dataset.column_names)\nprint(\"\\nSample from dataset:\", train_dataset[0])\n\n# Prepare labels with MultiLabelBinarizer\nmlb = MultiLabelBinarizer()\ntrain_labels = mlb.fit_transform(train_df['IPC_section'])\nval_labels = mlb.transform(val_df['IPC_section'])\ntest_labels = mlb.transform(test_df['IPC_section'])\n\nprint(\"\\nLabels shape:\", train_labels.shape)\nprint(\"Number of classes:\", len(mlb.classes_))\nprint(\"Sample classes:\", mlb.classes_[:5])\nprint(\"Sample label vector:\", train_labels[0])\n\n# Test adding labels to dataset\ntrain_labels_list = [list(label) for label in train_labels]\nsample_dataset = train_dataset.add_column(\"labels\", train_labels_list)\nprint(\"\\nAfter adding labels:\", \"labels\" in sample_dataset.column_names)\nprint(\"Sample with labels:\", sample_dataset[0]['labels'])\n\n# Test tokenizer and model setup\nmodel_name = \"prajjwal1/bert-tiny\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Create a simple preprocess function for testing\ndef simple_preprocess(examples):\n    return tokenizer(examples['Case_Description'], truncation=True, padding='max_length', max_length=128)\n\n# Test preprocessing on a small subset\nsmall_dataset = Dataset.from_dict({\"Case_Description\": train_df['Case_Description'].head(5).tolist()})\nprocessed = small_dataset.map(simple_preprocess, batched=True)\nprint(\"\\nAfter preprocessing:\", processed.column_names)\nprint(\"Sample processed item:\", processed[0])\n\n# Test combining preprocessing and labels\nsmall_train_df = train_df.head(5)\nsmall_train_dataset = Dataset.from_pandas(small_train_df)\nsmall_train_labels = mlb.transform(small_train_df['IPC_section'])\nsmall_train_labels_list = [list(label) for label in small_train_labels]\n\nsmall_processed = small_train_dataset.map(simple_preprocess, batched=True)\nsmall_processed_with_labels = small_processed.add_column(\"labels\", small_train_labels_list)\n\nprint(\"\\nAfter preprocessing and adding labels:\", small_processed_with_labels.column_names)\nprint(\"Sample item with features and labels:\", {k: v for k, v in small_processed_with_labels[0].items() if k in ['input_ids', 'labels']})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T13:23:30.817043Z","iopub.execute_input":"2025-04-04T13:23:30.817427Z","iopub.status.idle":"2025-04-04T13:25:41.080958Z","shell.execute_reply.started":"2025-04-04T13:23:30.817388Z","shell.execute_reply":"2025-04-04T13:25:41.079555Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import spacy\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom transformers import DataCollatorWithPadding\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom datasets import Dataset\nimport joblib\nimport os\n\n# 1. Load the dataset\nprint(\"Loading dataset...\")\ndf = pd.read_csv(\"/kaggle/input/ipc-section/Balanced_IPC_Sections_409_Cleaned.csv\")\ndf = df[['Case_Description', 'IPC_section']]\n\n# 2. Encode labels (single-label classification)\nprint(\"Encoding labels...\")\nlabel_encoder = LabelEncoder()\ndf['IPC_section_encoded'] = label_encoder.fit_transform(df['IPC_section'])\n\n# 3. Split the dataset\nprint(\"Splitting dataset...\")\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\nval_df, test_df = train_test_split(test_df, test_size=0.5, random_state=42)\n\n# 4. Create datasets (with encoded labels)\ntrain_dataset = Dataset.from_pandas(train_df[['Case_Description', 'IPC_section_encoded']])\nval_dataset = Dataset.from_pandas(val_df[['Case_Description', 'IPC_section_encoded']])\ntest_dataset = Dataset.from_pandas(test_df[['Case_Description', 'IPC_section_encoded']])\n\n# 5. Load NLP and tokenizer\nprint(\"Loading NLP model and tokenizer...\")\ntry:\n    # Load with minimal components for speed\n    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"lemmatizer\"])\nexcept:\n    # If model isn't available, download it\n    print(\"Downloading spaCy model...\")\n    spacy.cli.download(\"en_core_web_sm\")\n    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"lemmatizer\"])\n\n# Tokenizer initialization for TinyBERT\nmodel_name = \"prajjwal1/bert-tiny\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# 6. Define preprocessing function (simplified for speed)\ndef extract_legal_features(text):\n    # Process text with spaCy\n    doc = nlp(text)\n    \n    # Extract named entities (simple version)\n    entities = [f\"{ent.label_}:{ent.text}\" for ent in doc.ents]\n    \n    # Extract key noun phrases (simple version)\n    legal_terms = []\n    for chunk in doc.noun_chunks:\n        if any(legal_term in chunk.text.lower() for legal_term in \n              [\"law\", \"court\", \"plaintiff\", \"defendant\", \"section\", \"act\", \"offense\"]):\n            legal_terms.append(f\"LEGAL:{chunk.text}\")\n    \n    # Combine features\n    legal_features = \" \".join(entities + legal_terms)\n    return legal_features\n\ndef preprocess_function(examples, batch_size=16):\n    \"\"\"Process in smaller batches for memory efficiency\"\"\"\n    case_desc_batch = examples['Case_Description']\n    enriched_texts = []\n    \n    # Process in sub-batches\n    for i in range(0, len(case_desc_batch), batch_size):\n        sub_batch = case_desc_batch[i:i+batch_size]\n        for text in sub_batch:\n            # Extract features\n            legal_features = extract_legal_features(text)\n            # Combine with original text\n            enriched_text = text + \" [LEGAL] \" + legal_features\n            enriched_texts.append(enriched_text)\n    \n    # Tokenize\n    return tokenizer(enriched_texts, truncation=True, padding='max_length', max_length=128)\n\n# 7. Apply preprocessing\nprint(\"Preprocessing train dataset...\")\ntrain_dataset_processed = train_dataset.map(\n    preprocess_function, \n    batched=True,\n    num_proc=2,  # Adjust based on your system\n)\n\nprint(\"Preprocessing validation dataset...\")\nval_dataset_processed = val_dataset.map(\n    preprocess_function, \n    batched=True,\n    num_proc=2,\n)\n\nprint(\"Preprocessing test dataset...\")\ntest_dataset_processed = test_dataset.map(\n    preprocess_function, \n    batched=True,\n    num_proc=2,\n)\n\n# 8. Add labels to processed datasets\nprint(\"Adding labels to datasets...\")\ntrain_dataset_final = train_dataset_processed.add_column(\"labels\", train_df['IPC_section_encoded'].tolist())\nval_dataset_final = val_dataset_processed.add_column(\"labels\", val_df['IPC_section_encoded'].tolist())\ntest_dataset_final = test_dataset_processed.add_column(\"labels\", test_df['IPC_section_encoded'].tolist())\n\n# 9. Verify labels are present\nprint(f\"Train dataset has labels: {'labels' in train_dataset_final.column_names}\")\nprint(f\"Sample label: {train_dataset_final[0]['labels']}...\")  # Show first 5 values\n\n# 10. Model initialization\nprint(\"Initializing model...\")\n# Initialize from scratch with configuration\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name, \n    num_labels=len(label_encoder.classes_),  # Number of unique IPC sections\n    problem_type=\"single_label_classification\"  # Single-label classification\n)\n\n# 11. Define compute metrics function\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    # Apply softmax for single-label classification\n    probs = torch.nn.functional.softmax(torch.tensor(predictions), dim=-1)\n    predictions = torch.argmax(probs, dim=-1)\n    \n    # Convert boolean comparison to integer type (1 for True, 0 for False)\n    accuracy = (predictions == labels).float().mean().item()  # Convert to float for mean calculation\n    \n    return {\"accuracy\": accuracy}\n\n# 12. Training arguments\nprint(\"Setting up training arguments...\")\ntraining_args = TrainingArguments(\n    output_dir=\"./ipc_tinybert_scratch\",\n    evaluation_strategy=\"steps\",\n    eval_steps=100,\n    save_strategy=\"steps\",\n    save_steps=100,\n    learning_rate=5e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=32,\n    num_train_epochs=5,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    logging_steps=50,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    greater_is_better=True,\n    fp16=False,  # Set to True if you have GPU support\n    gradient_accumulation_steps=2,\n    report_to=\"none\"\n)\n\n# 13. Data collator\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n# 14. Initialize trainer (standard version first)\nprint(\"Initializing trainer...\")\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset_final,\n    eval_dataset=val_dataset_final,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\n# 15. Train the model\nprint(\"Starting training...\")\ntrainer.train()\n\n# 16. Evaluate on test set\nprint(\"Evaluating on test set...\")\ntest_results = trainer.evaluate(test_dataset_final)\nprint(f\"Test results: {test_results}\")\n\n# 17. Save the model and related files\nmodel_path = \"./ipc_legal_context_model_single_label\"\nprint(f\"Saving model to {model_path}...\")\nmodel.save_pretrained(model_path)\ntokenizer.save_pretrained(model_path)\njoblib.dump(label_encoder, os.path.join(model_path, \"label_encoder.pkl\"))\n\nprint(\"Training complete!\")\n\n# 18. Example prediction function\ndef predict_ipc_sections(case_text, model, tokenizer, label_encoder):\n    # Extract legal features\n    legal_features = extract_legal_features(case_text)\n    enriched_text = case_text + \" [LEGAL] \" + legal_features\n    \n    # Tokenize\n    inputs = tokenizer(enriched_text, truncation=True, padding='max_length', \n                     max_length=128, return_tensors=\"pt\")\n    \n    # Make prediction\n    model.eval()\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n        probs = torch.softmax(logits, dim=-1)\n        prediction = torch.argmax(probs, dim=-1).item()\n    \n    # Convert to IPC section\n    predicted_section = label_encoder.inverse_transform([[prediction]])[0][0]\n    return predicted_section, probs.cpu().numpy()[0]\n\nprint(\"\\nCode execution complete!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T13:44:12.974177Z","iopub.execute_input":"2025-04-04T13:44:12.974486Z","iopub.status.idle":"2025-04-04T13:47:02.915153Z","shell.execute_reply.started":"2025-04-04T13:44:12.974464Z","shell.execute_reply":"2025-04-04T13:47:02.914365Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\nimport joblib\n\n# Load the saved model and tokenizer\nmodel_path = \"./ipc_legal_context_model_single_label\"\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nlabel_encoder = joblib.load(f\"{model_path}/label_encoder.pkl\")\n\n# Example case descriptions for prediction\nexample_cases = [\n    \"A person was caught driving under the influence of alcohol and caused an accident.\",\n    \"A group of thieves robbed a bank using guns and hostages.\",\n    \"A man was arrested for selling fake medicines without a license.\",\n    \"The defendant failed to appear in court on the scheduled trial date.\",\n    \"A woman was assaulted and harassed by a stranger in public.\"\n]\n\n# Function to predict IPC sections\ndef predict_ipc_sections(case_text, model, tokenizer, label_encoder):\n    # Tokenize the input text\n    inputs = tokenizer(case_text, truncation=True, padding='max_length', max_length=128, return_tensors=\"pt\")\n    \n    # Predict with the model\n    model.eval()\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n        probs = torch.softmax(logits, dim=-1)  # Get probabilities from logits\n        prediction = torch.argmax(probs, dim=-1).item()  # Get the predicted IPC section index\n    \n    # Convert to IPC section\n    predicted_section = label_encoder.inverse_transform([[prediction]])[0][0]\n    confidence = probs.cpu().numpy()[0][prediction]  # Get confidence for the prediction\n    \n    return predicted_section, confidence\n\n# Predict for each example case and display results\nfor i, case in enumerate(example_cases):\n    predicted_section, confidence = predict_ipc_sections(case, model, tokenizer, label_encoder)\n    print(f\" Sample {i + 1}:\")\n    print(f\" Text: {case}\")\n    print(f\" Predicted IPC Section: {predicted_section}\")\n    print(f\" Confidence: {confidence:.4f}\")\n    print(\"\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T13:49:10.201889Z","iopub.execute_input":"2025-04-04T13:49:10.202277Z","iopub.status.idle":"2025-04-04T13:49:10.598744Z","shell.execute_reply.started":"2025-04-04T13:49:10.202246Z","shell.execute_reply":"2025-04-04T13:49:10.597818Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import spacy\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom transformers import DataCollatorWithPadding\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom datasets import Dataset\nimport joblib\nimport os\n\n# 1. Load the dataset\nprint(\"Loading dataset...\")\ndf = pd.read_csv(\"/kaggle/input/ipc-section/Balanced_IPC_Sections_409_Cleaned.csv\")\ndf = df[['Case_Description', 'IPC_section']]\n\n# 2. Encode labels (single-label classification)\nprint(\"Encoding labels...\")\nlabel_encoder = LabelEncoder()\ndf['IPC_section_encoded'] = label_encoder.fit_transform(df['IPC_section'])\n\n# 3. Split the dataset\nprint(\"Splitting dataset...\")\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\nval_df, test_df = train_test_split(test_df, test_size=0.5, random_state=42)\n\n# 4. Create datasets (with encoded labels)\ntrain_dataset = Dataset.from_pandas(train_df[['Case_Description', 'IPC_section_encoded']])\nval_dataset = Dataset.from_pandas(val_df[['Case_Description', 'IPC_section_encoded']])\ntest_dataset = Dataset.from_pandas(test_df[['Case_Description', 'IPC_section_encoded']])\n\n# 5. Load NLP and tokenizer\nprint(\"Loading NLP model and tokenizer...\")\ntry:\n    # Load with minimal components for speed\n    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"lemmatizer\"])\nexcept:\n    # If model isn't available, download it\n    print(\"Downloading spaCy model...\")\n    spacy.cli.download(\"en_core_web_sm\")\n    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"lemmatizer\"])\n\n# Tokenizer initialization for DistilBERT\nmodel_name = \"distilbert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# 6. Define preprocessing function (simplified for speed)\ndef extract_legal_features(text):\n    # Process text with spaCy\n    doc = nlp(text)\n    \n    # Extract named entities (simple version)\n    entities = [f\"{ent.label_}:{ent.text}\" for ent in doc.ents]\n    \n    # Extract key noun phrases (simple version)\n    legal_terms = []\n    for chunk in doc.noun_chunks:\n        if any(legal_term in chunk.text.lower() for legal_term in \n              [\"law\", \"court\", \"plaintiff\", \"defendant\", \"section\", \"act\", \"offense\"]):\n            legal_terms.append(f\"LEGAL:{chunk.text}\")\n    \n    # Combine features\n    legal_features = \" \".join(entities + legal_terms)\n    return legal_features\n\ndef preprocess_function(examples, batch_size=16):\n    \"\"\"Process in smaller batches for memory efficiency\"\"\"\n    case_desc_batch = examples['Case_Description']\n    enriched_texts = []\n    \n    # Process in sub-batches\n    for i in range(0, len(case_desc_batch), batch_size):\n        sub_batch = case_desc_batch[i:i+batch_size]\n        for text in sub_batch:\n            # Extract features\n            legal_features = extract_legal_features(text)\n            # Combine with original text\n            enriched_text = text + \" [LEGAL] \" + legal_features\n            enriched_texts.append(enriched_text)\n    \n    # Tokenize\n    return tokenizer(enriched_texts, truncation=True, padding='max_length', max_length=128)\n\n# 7. Apply preprocessing\nprint(\"Preprocessing train dataset...\")\ntrain_dataset_processed = train_dataset.map(\n    preprocess_function, \n    batched=True,\n    num_proc=2,  # Adjust based on your system\n)\n\nprint(\"Preprocessing validation dataset...\")\nval_dataset_processed = val_dataset.map(\n    preprocess_function, \n    batched=True,\n    num_proc=2,\n)\n\nprint(\"Preprocessing test dataset...\")\ntest_dataset_processed = test_dataset.map(\n    preprocess_function, \n    batched=True,\n    num_proc=2,\n)\n\n# 8. Add labels to processed datasets\nprint(\"Adding labels to datasets...\")\ntrain_dataset_final = train_dataset_processed.add_column(\"labels\", train_df['IPC_section_encoded'].tolist())\nval_dataset_final = val_dataset_processed.add_column(\"labels\", val_df['IPC_section_encoded'].tolist())\ntest_dataset_final = test_dataset_processed.add_column(\"labels\", test_df['IPC_section_encoded'].tolist())\n\n# 9. Verify labels are present\nprint(f\"Train dataset has labels: {'labels' in train_dataset_final.column_names}\")\nprint(f\"Sample label: {train_dataset_final[0]['labels']}...\")  # Show first 5 values\n\n# 10. Model initialization\nprint(\"Initializing model...\")\n# Initialize DistilBERT model from scratch\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name, \n    num_labels=len(label_encoder.classes_),  # Number of unique IPC sections\n    problem_type=\"single_label_classification\"  # Single-label classification\n)\n\n# 11. Define compute metrics function\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    # Apply softmax for single-label classification\n    probs = torch.nn.functional.softmax(torch.tensor(predictions), dim=-1)\n    predictions = torch.argmax(probs, dim=-1)\n    \n    # Convert boolean comparison to integer type (1 for True, 0 for False)\n    accuracy = (predictions == labels).float().mean().item()  # Convert to float for mean calculation\n    \n    return {\"accuracy\": accuracy}\n\n# 12. Training arguments\nprint(\"Setting up training arguments...\")\ntraining_args = TrainingArguments(\n    output_dir=\"./ipc_distilbert_scratch\",\n    evaluation_strategy=\"steps\",\n    eval_steps=100,\n    save_strategy=\"steps\",\n    save_steps=100,\n    learning_rate=5e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=32,\n    num_train_epochs=5,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    logging_steps=50,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    greater_is_better=True,\n    fp16=False,  # Set to True if you have GPU support\n    gradient_accumulation_steps=2,\n    report_to=\"none\"\n)\n\n# 13. Data collator\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n# 14. Initialize trainer (standard version first)\nprint(\"Initializing trainer...\")\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset_final,\n    eval_dataset=val_dataset_final,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\n# 15. Train the model\nprint(\"Starting training...\")\ntrainer.train()\n\n# 16. Evaluate on test set\nprint(\"Evaluating on test set...\")\ntest_results = trainer.evaluate(test_dataset_final)\nprint(f\"Test results: {test_results}\")\n\n# 17. Save the model and related files\nmodel_path = \"./ipc_legal_context_model_distilbert\"\nprint(f\"Saving model to {model_path}...\")\nmodel.save_pretrained(model_path)\ntokenizer.save_pretrained(model_path)\njoblib.dump(label_encoder, os.path.join(model_path, \"label_encoder.pkl\"))\n\nprint(\"Training complete!\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T14:10:57.934123Z","iopub.execute_input":"2025-04-04T14:10:57.934438Z","iopub.status.idle":"2025-04-04T14:25:22.004951Z","shell.execute_reply.started":"2025-04-04T14:10:57.934409Z","shell.execute_reply":"2025-04-04T14:25:22.003935Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\nimport joblib\n\n# Load the saved model and tokenizer\nmodel_path = \"./ipc_legal_context_model_distilbert\"  # Path to the saved model\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nlabel_encoder = joblib.load(f\"{model_path}/label_encoder.pkl\")\n\n# Example case descriptions for prediction\nexample_cases = [\n    \"A person was caught driving under the influence of alcohol and caused an accident.\",\n    \"A group of thieves robbed a bank using guns and hostages.\",\n    \"A man was arrested for selling fake medicines without a license.\",\n    \"The defendant failed to appear in court on the scheduled trial date.\",\n    \"A woman was assaulted and harassed by a stranger in public.\"\n]\n\n# Function to predict IPC sections\ndef predict_ipc_sections(case_text, model, tokenizer, label_encoder):\n    # Tokenize the input text\n    inputs = tokenizer(case_text, truncation=True, padding='max_length', max_length=128, return_tensors=\"pt\")\n    \n    # Make prediction\n    model.eval()\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n        probs = torch.softmax(logits, dim=-1)  # Get probabilities from logits\n        prediction = torch.argmax(probs, dim=-1).item()  # Get the predicted IPC section index\n    \n    # Convert to IPC section\n    predicted_section = label_encoder.inverse_transform([[prediction]])[0][0]\n    confidence = probs.cpu().numpy()[0][prediction]  # Get confidence for the prediction\n    \n    return predicted_section, confidence\n\n# Predict for each example case and display results\nfor i, case in enumerate(example_cases):\n    predicted_section, confidence = predict_ipc_sections(case, model, tokenizer, label_encoder)\n    print(f\" Sample {i + 1}:\")\n    print(f\" Text: {case}\")\n    print(f\" Predicted IPC Section: {predicted_section}\")\n    print(f\" Confidence: {confidence:.4f}\")\n    print(\"\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T14:25:42.985123Z","iopub.execute_input":"2025-04-04T14:25:42.985439Z","iopub.status.idle":"2025-04-04T14:25:43.573995Z","shell.execute_reply.started":"2025-04-04T14:25:42.985416Z","shell.execute_reply":"2025-04-04T14:25:43.573092Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\nimport joblib\n\n# Load the saved model and tokenizer\nmodel_path = \"./ipc_legal_context_model_distilbert\"  # Path to the saved model\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nlabel_encoder = joblib.load(f\"{model_path}/label_encoder.pkl\")\n\n# Example case descriptions for prediction\nexample_cases = [\n    \"A person was caught driving under the influence of alcohol and caused an accident.\",\n    \"A group of thieves robbed a bank using guns and hostages.\",\n    \"A man was arrested for selling fake medicines without a license.\",\n    \"The defendant failed to appear in court on the scheduled trial date.\",\n    \"A woman was assaulted and harassed by a stranger in public.\"\n]\n\n# Function to predict IPC sections\ndef predict_ipc_sections(case_text, model, tokenizer, label_encoder):\n    # Tokenize the input text\n    inputs = tokenizer(case_text, truncation=True, padding='max_length', max_length=128, return_tensors=\"pt\")\n    \n    # Make prediction\n    model.eval()\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n        probs = torch.softmax(logits, dim=-1)  # Get probabilities from logits\n        prediction = torch.argmax(probs, dim=-1).item()  # Get the predicted IPC section index\n    \n    # Convert to IPC section\n    predicted_section = label_encoder.inverse_transform([[prediction]])[0][0]  # Correctly map back to IPC section\n    confidence = probs.cpu().numpy()[0][prediction]  # Get confidence for the prediction\n    \n    return predicted_section, confidence\n\n# Predict for each example case and display results\nfor i, case in enumerate(example_cases):\n    predicted_section, confidence = predict_ipc_sections(case, model, tokenizer, label_encoder)\n    print(f\" Sample {i + 1}:\")\n    print(f\" Text: {case}\")\n    print(f\" Predicted IPC Section: {predicted_section}\")\n    print(f\" Confidence: {confidence:.4f}\")\n    print(\"\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T14:27:02.314007Z","iopub.execute_input":"2025-04-04T14:27:02.314352Z","iopub.status.idle":"2025-04-04T14:27:02.746189Z","shell.execute_reply.started":"2025-04-04T14:27:02.314327Z","shell.execute_reply":"2025-04-04T14:27:02.745141Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T16:07:41.742258Z","iopub.status.idle":"2025-04-04T16:07:41.742684Z","shell.execute_reply":"2025-04-04T16:07:41.742565Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_fscore_support\n\ndef compute_metrics(p):\n    predictions, labels = p\n    preds = predictions.argmax(axis=-1)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n    accuracy = accuracy_score(labels, preds)\n    return {\n        \"accuracy\": accuracy,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T17:17:45.142588Z","iopub.execute_input":"2025-04-04T17:17:45.142876Z","iopub.status.idle":"2025-04-04T17:17:45.147316Z","shell.execute_reply.started":"2025-04-04T17:17:45.142854Z","shell.execute_reply":"2025-04-04T17:17:45.146642Z"}},"outputs":[],"execution_count":null}]}